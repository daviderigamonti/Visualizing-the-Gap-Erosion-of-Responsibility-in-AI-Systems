\section{Contrasting Viewpoints}\label{sec:viewpoints}

% Introduction
Through a comprehensive analysis of contrasting viewpoints we can shed light on the issues of our thesis and hopefully reach solid conclusions.

% Deflationism evolution into recognizing the gap
One of the earlier formulations of \textit{deflationism} \parencite{HAYTMP} that was analyzed in the previous section clearly goes against our main thesis as it refuses to acknowledge the responsibility gap as meaningful.
However, the deflationist ideology evolved over time to accept the notion of responsibility gap and focus on other aspects of its original theory \parencite{SIJWA}; these result grants some empirical credibility to the foundation of our thesis.

% Legal counterargument
A possible counterargument to the main thesis is that trying to construct a new framework with new rules is superfluous as the classical one can be adapted to accommodate autonomous systems with the help of the inherent flexibility of law when interpreted by a court.
The previous claim makes some strong assumptions on the quality, effectiveness and homogeneity of law systems around the world and across different cultures.
However, our main thesis still holds since we would like to approach the issue on an ethical level of thought, where legal liability is just a part of the bigger picture.

% Explainability tools
Another important point to delve into is the consideration that there exists a variety of explainability tools capable of providing insight into the AI decision-making process.
One example of such a tool is Grad-CAM \parencite{RAMGCW}, which is specifically designed for deep neural networks dedicated to image processing applications.
While it would be fair to say that autonomous systems are not entirely black boxes given the existence of these tools that can partially demystify the inference process of some AI models, there are still some major flaws to this argument.
Firstly, analyzing the decision-making process of an autonomous system on a case-by-case scenario is cumbersome and also impossible if the system has to take split-second decision (as it may be the case with a LAWS system), thus relegating their use only to support passive responsibility audits.
Even if we wanted to use explainability tools to assist the designers of an autonomous system to perfect its decisions, we could never percieve the system as an absolute white box due to the fact that the primary appeal of a learning autonomous system is strictly correlated to its inherent opacity.
This opacity arises from the fact that we want to solve tasks that are either too complex or too difficult to express in a formal way.
Furthermore, having full knowledge regarding the decision making process of an AI system would imply that we could completely replicate it by using a tree of if-else statements, which is a deterministic process, thus reliable; obviously, this is under the assumption that the training and inference steps of the system are separate (not applicable to scenarios similar to reinforcement learning).