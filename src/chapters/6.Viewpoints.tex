\section{Contrasting Viewpoints}\label{sec:viewpoints}

% Introduction
Through a comprehensive analysis of contrasting viewpoints we can shed light on the issues of our thesis and hopefully reach solid conclusions.

% Deflationism evolution into recognizing the gap
One of the earlier formulations of \textit{deflationism} \parencite{HAYTMP} that were analyzed in the previous section clearly goes against our main thesis as it refuses to acknowledge the responsibility gap as meaningful.
However, the deflationist ideology evolved over time to accept the notion of responsibility gap and focus on other aspects of its original theory \parencite{SIJWA}; this result grants some empirical credibility to the foundation of our thesis.

% Legal counterargument
A possible counterargument to the main thesis is that trying to construct a new framework with new rules is superfluous as the classical one can be adapted to accomodate autonomous systems with the help of the law's inherent flexibility when interpreted by a court.
The previous claim makes some strong assumptions on the quality, effectiveness and homogeneity of law systems around the world and across different cultures, however our main thesis still holds since we would like to approach the issue on an ethical level of thought, where legal liability is only a part of the bigger picture.

Another important point to delve into is the consideration that exists a variety of explainability tools capable of giving insight into the AI decision making process.
An example of such a tool is Grad-CAM \parencite{RAMGCW}, which is specifically designed for deep neural networks dedicated to image processing applications.
Given the existence of this tools, one could say that autonomous systems aren't actually black boxes; while this observation is completely fair as explainability tools can partially demistify the inferring process of some AI models, there are some major flaws to this argument.
First of all, analyzing the decision process of an autonomous system on a case-by-case scenario is cumbersome and also impossbile if the system has to take split-second decision (as it could be the case of a LAWS system), thus relegating their use only to support passive responsibility audits.
Even if we wanted to use explainability tools to aid the designers of a autonomous system to perfect its decisions, we could never see the system as a white box due to the fact that the main appeal of a learning autonomous system is strictly correlated to its inherent opacity since we want to solve task that are either too complex or too difficult to express in a formal way.
In addition, having full knowledge of the decision making process of an AI system would mean that we could completely replicate it by using a tree of if-else statements, which is a deterministic process, thus reliable; obviously, this is undert the assumption that the training and the inference steps of the system are separate (not applicable to scenarios similar to reinforcement learning).