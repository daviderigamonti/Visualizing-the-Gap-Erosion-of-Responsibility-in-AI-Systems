@book{ALTAI,
	author = {European Commission and Directorate-General for Communications Networks, Content and Technology},
	title = {The Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self assessment},
	publisher = {Publications Office},
	year = {2020},
	doi = {doi/10.2759/002360}
}

@article{COEAIR,
	author = {Mark Coeckelbergh},
	pages = {2051--2068},
	title = {Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability},
	publisher = {Springer Verlag},
	journal = {Science and Engineering Ethics},
	year = {2020},
	doi = {10.1007/s11948-019-00146-8},
	number = {4},
	volume = {26}
}

@article{CONURI,
	doi = {10.1007/s10676-021-09616-9},
	publisher = {Springer Verlag},
	pages = {803--814},
	title = {Understanding Responsibility in Responsible Ai. Dianoetic Virtues and the Hard Problem of Context},
	journal = {Ethics and Information Technology},
	author = {Mihaela Constantinescu and Cristina Voinea and Radu Uszkai and Constantin Vic\ua},
	volume = {23},
	year = {2021},
	number = {4}
}

@Inbook{DURAPF,
	author = "Durante, Massimo and Floridi, Luciano",
	editor = "M{\"o}kander, Jakob and Ziosi, Marta",
	title = "A Legal Principles-Based Framework for AI Liability Regulation",
	bookTitle = "The 2021 Yearbook of the Digital Ethics Lab",
	year = "2022",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "93--112",
	abstract = "Europe has recently taken the path of regulating artificial intelligence (AI). This is a complex task, in which it is crucial to understand what the purposes of regulation are. In this perspective, it is not enough to identify and set ethical guidelines and legal norms. It is also important to envisage the legal principles that might steer the regulation of AI, which is aimed to reconcile technological innovation, economic development and user trust. Therefore, it may be useful to consider whether some principles may emerge from existing legislation in the AI sector. To this aim, we review the work of the Expert Group on Product Liability in the field of AI and Emerging Technologies (2019) as a case study. We show how their work has started to lay the basis for a set of legal principles for AI liability regime. An initial and open list of legal principles can serve as a benchmark for future work on a principles-based AI regulation.",
	isbn = "978-3-031-09846-8",
	doi = "10.1007/978-3-031-09846-8_7",
	url = "https://doi.org/10.1007/978-3-031-09846-8_7"
}

@book{ETE,
	title = "Ethics, technology, and engineering : an introduction",
	abstract = "Featuring a wide range of international case studies, Ethics, Technology, and Engineering presents a unique and systematic approach for engineering students to deal with the ethical issues that are increasingly inherent in engineering practice.",
	author = "{Poel, van de}, I.R. and L.M.M. Royakkers",
	year = "2011",
	language = "English",
	isbn = "978-1-4443-3095-3",
	publisher = "Wiley-Blackwell",
	address = "United States",
}

@article{MATTRG,
	doi = {10.1007/s10676-004-3422-1},
	title = {The Responsibility Gap: Ascribing Responsibility for the Actions of Learning Automata},
	pages = {175--183},
	year = {2004},
	number = {3},
	publisher = {Kluwer Academic Publishers},
	journal = {Ethics and Information Technology},
	author = {Andreas Matthias},
	volume = {6}
}

@article{NISAIA,
	number = {1},
	publisher = {Springer},
	doi = {10.1007/bf02639315},
	volume = {2},
	author = {Helen Nissenbaum},
	pages = {25--42},
	journal = {Science and Engineering Ethics},
	year = {1996},
	title = {Accountability in a Computerized Society}
}

@article{NOVAIA,
	journal = {Ai and Society: Knowledge, Culture and Communication},
	title = {Accountability in Artificial Intelligence: What It is and How It Works},
	author = {Claudio Novelli and Mariarosaria Taddeo and Luciano Floridi},
	year = {forthcoming}
}

@incollection{OLJODA,
    author = {Olsen, Johan P.},
    isbn = {9780198800606},
    title = "{74Ambiguity and the Politics of Accountability}",
    booktitle = "{Democratic Accountability, Political Order, and Change: Exploring Accountability Processes in an Era of European Transformation}",
    publisher = {Oxford University Press},
    year = {2017},
    month = {04},
    abstract = "{In mainstream principal-agent approaches, as well as in much democratic theory and organization theory, accountability is linked to a belief in human agency and history determined by human will, causal understanding, and control. An institutional approach considers the possibility that events are not necessarily a product of the deliberate choices of identifiable actors, and takes into account that ambiguity, uncertainty, and limited control are inherent to political and organizational decision making. The fluidity and unresolved conflicts of political life make it difficult to conclude who is responsible and should be held to account and learn from experience and there is more to accountability processes than decision making, control, and compliance. Ambiguity and uncertainty about the past—what has happened, why, and who is responsible and should be held to account—open the way for the politics of accountability, involving sense-making processes, competing interpretations, and coping with conflict.}",
    doi = {10.1093/acprof:oso/9780198800606.003.0004},
    url = {https://doi.org/10.1093/acprof:oso/9780198800606.003.0004},
    eprint = {https://academic.oup.com/book/0/chapter/153440508/chapter-ag-pdf/44965507/book\_8053\_section\_153440508.ag.pdf},
}

@article{SANFRG,
	volume = {34},
	journal = {Philosophy and Technology},
	author = {Filippo Santoni de Sio and Giulio Mecacci},
	publisher = {Springer Verlag},
	number = {4},
	pages = {1057--1084},
	year = {2021},
	title = {Four Responsibility Gaps with Artificial Intelligence: Why They Matter and How to Address Them},
	doi = {10.1007/s13347-021-00450-x}
}

@article{SARER,
	pages = {1--10},
	title = {Engineering Responsibility},
	volume = {24},
	journal = {Ethics and Information Technology},
	publisher = {Springer Verlag},
	number = {3},
	year = {2022},
	doi = {10.1007/s10676-022-09660-z},
	author = {Nicholas Sars}
}

@article{SPAKR,
	doi = {10.1111/j.1468-5930.2007.00346.x},
	pages = {62--77},
	volume = {24},
	year = {2007},
	publisher = {Wiley-Blackwell},
	author = {Robert Sparrow},
	title = {Killer Robots},
	journal = {Journal of Applied Philosophy},
	number = {1}
}

@article{STARCA,
	doi = {10.1007/s10676-006-9112-4},
	volume = {8},
	title = {Responsible Computers? A Case for Ascribing Quasi-Responsibility to Computers Independent of Personhood or Agency},
	author = {Bernd Carsten Stahl},
	journal = {Ethics and Information Technology},
	pages = {205--213},
	number = {4},
	publisher = {Springer},
	year = {2006}
}

@article{VERAAC,
	doi = {10.1007/s11023-020-09532-9},
	year = {2020},
	volume = {31},
	author = {Ilse Verdiesen and Filippo Santoni de Sio and Virginia Dignum},
	pages = {137--163},
	publisher = {Springer Verlag},
	title = {Accountability and Control Over Autonomous Weapon Systems: A Framework for Comprehensive Human Oversight},
	journal = {Minds and Machines},
	number = {1}
}

@inbook{FEISC,
	author = {Feinberg, Joel},
	title = {Sua Culpa},
	year = {1985},
	isbn = {0534042570},
	publisher = {Wadsworth Publ. Co.},
	address = {USA},
	booktitle = {Ethical Issues in the Use of Computers},
	pages = {102–120},
	numpages = {19}
}

@book{FISRAC,
	year = {1998},
	publisher = {New York: Cambridge University Press},
	editor = {Mark Ravizza},
	author = {John Martin Fischer and Mark Ravizza},
	title = {Responsibility and Control: A Theory of Moral Responsibility}
}


@article{POPOM,
	title = "The problem of many hands : climate change as an example",
	abstract = "In some situations in which undesirable collective effects occur, it is very hard, if not impossible, to hold any individual reasonably responsible. Such a situation may be referred to as the problem of many hands. In this paper we investigate how the problem of many hands can best be understood and why, and when, it exactly constitutes a problem. After analyzing climate change as an example, we propose to define the problem of many hands as the occurrence of a gap in the distribution of responsibility that may be considered morally problematic. Whether a gap is morally problematic, we suggest, depends on the reasons why responsibility is distributed. This, in turn, depends, at least in part, on the sense of responsibility employed, a main distinction being that between backward-looking and forward-looking responsibility.",
	author = "{Poel, van de}, I.R. and J.N. Fahlquist and N. Doorn and S.D. Zwart and L.M.M. Royakkers",
	year = "2012",
	doi = "10.1007/s11948-011-9276-0",
	language = "English",
	volume = "18",
	pages = "49--67",
	journal = "Science and Engineering Ethics",
	issn = "1353-3452",
	publisher = "Springer",
	number = "1",
}

@article{THETE,
	author = {Theodorou, Andreas and Dignum, Virginia},
	year = {2020},
	month = {01},
	pages = {1-3},
	title = {Towards ethical and socio-legal governance in AI},
	volume = {2},
	journal = {Nature Machine Intelligence},
	doi = {10.1038/s42256-019-0136-y}
}

@book{MORTSE,
	title = {To Save Everything, Click Here: The Folly of Technological Solutionism},
	author = {Morozov, E.},
	isbn = {9781610391399},
	lccn = {2012042279},
	url = {https://books.google.it/books?id=fdggBahA1qsC},
	year = {2013},
	publisher = {PublicAffairs}
}

@article{STILML,
	author = {Jack Stilgoe},
	title = {Machine learning, social learning and the governance of self-driving cars},
	journal = {Social Studies of Science},
	volume = {48},
	number = {1},
	pages = {25-56},
	year = {2018},
	doi = {10.1177/0306312717741687},
	note = {PMID: 29160165},
	url = {https://doi.org/10.1177/0306312717741687},
	eprint = {https://doi.org/10.1177/0306312717741687},
	abstract = { Self-driving cars, a quintessentially ‘smart’ technology, are not born smart. The algorithms that control their movements are learning as the technology emerges. Self-driving cars represent a high-stakes test of the powers of machine learning, as well as a test case for social learning in technology governance. Society is learning about the technology while the technology learns about society. Understanding and governing the politics of this technology means asking ‘Who is learning, what are they learning and how are they learning?’ Focusing on the successes and failures of social learning around the much-publicized crash of a Tesla Model S in 2016, I argue that trajectories and rhetorics of machine learning in transport pose a substantial governance challenge. ‘Self-driving’ or ‘autonomous’ cars are misnamed. As with other technologies, they are shaped by assumptions about social needs, solvable problems, and economic opportunities. Governing these technologies in the public interest means improving social learning by constructively engaging with the contingencies of machine learning. }
}

@article{HAYTMP,
	author = {Hayenhjelm, Madeleine and Wolff, Jonathan},
	title = {The Moral Problem of Risk Impositions: A Survey of the Literature},
	journal = {European Journal of Philosophy},
	volume = {20},
	number = {S1},
	pages = {E26-E51},
	doi = {https://doi.org/10.1111/j.1468-0378.2011.00482.x},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0378.2011.00482.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1468-0378.2011.00482.x},
	abstract = {Abstract This paper surveys the current philosophical discussion of the ethics of risk imposition, placing it in the context of relevant work in psychology, economics and social theory. The central philosophical problem starts from the observation that it is not practically possible to assign people individual rights not to be exposed to risk, as virtually all activity imposes some risk on others. This is the ‘problem of paralysis’. However, the obvious alternative theory that exposure to risk is justified when its total benefits exceed its total costs faces the standard distributional challenges of consequentialism. Forms of contractualism have been proposed as a solution, but how exactly such theories can be formulated remains problematic, especially when confronted with the difficult cases of mass, novel, risk such as climate change.},
	year = {2012}
}

@article{SIJWA,
    author = {Simpson, Thomas W. and Müller, Vincent C.},
    title = "{Just War and Robots’ Killings}",
    journal = {The Philosophical Quarterly},
    volume = {66},
    number = {263},
    pages = {302-322},
    year = {2015},
    month = {08},
    abstract = "{May lethal autonomous weapons systems—‘killer robots’—be used in war? The majority of writers argue against their use, and those who have argued in favour have done so on a consequentialist basis. We defend the moral permissibility of killer robots, but on the basis of the non-aggregative structure of right assumed by Just War theory. This is necessary because the most important argument against killer robots, the responsibility trilemma proposed by Rob Sparrow, makes the same assumptions. We show that the crucial moral question is not one of responsibility. Rather, it is whether the technology can satisfy the requirements of fairness in the redistribution of risk. Not only is this possible in principle, but some killer robots will actually satisfy these requirements. An implication of our argument is that there is a public responsibility to regulate killer robots’ design and manufacture.}",
    issn = {0031-8094},
    doi = {10.1093/pq/pqv075},
    url = {https://doi.org/10.1093/pq/pqv075},
    eprint = {https://academic.oup.com/pq/article-pdf/66/263/302/7441189/pqv075.pdf},
}

@article{RAMGCW,
	author = {Ramprasaath R. Selvaraju and Abhishek Das and Ramakrishna Vedantam and Michael Cogswell and Devi Parikh and Dhruv Batra},
	title = {Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization},
	journal = {CoRR},
	volume = {abs/1610.02391},
	year = {2016},
	url = {http://arxiv.org/abs/1610.02391},
	eprinttype = {arXiv},
	eprint = {1610.02391},
	timestamp = {Mon, 13 Aug 2018 16:46:58 +0200},
	biburl = {https://dblp.org/rec/journals/corr/SelvarajuDVCPB16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ICRCEA,
	author = {International Committee of the Red Cross},
	title = {Ethics and autonomous weapon systems: An ethical basis for human control?},
	year = {2018},
	month = {04},
	url = {https://www.icrc.org/en/download/file/69961/icrc_ethics_and_autonomous_weapon_systems_report_3_april_2018.pdf}
}

@article{EKEMBS,
	author = {Ekelhof, Merel},
	title = {Moving Beyond Semantics on Autonomous Weapons: Meaningful Human Control in Operation},
	journal = {Global Policy},
	volume = {10},
	number = {3},
	pages = {343-348},
	doi = {https://doi.org/10.1111/1758-5899.12665},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12665},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1758-5899.12665},
	abstract = {Abstract Ongoing discussions about autonomous weapons typically share concerns of losing control and the potentially destabilizing consequences for global security. To the extent that there is any consensus among states, academics, NGOs and other commentators involved in diplomatic efforts under the auspices of the UN Convention on Certain Conventional Weapons, it is grounded in the idea that all weapons should be subject to meaningful human control. This intuitively appealing concept immediately gained traction, although at a familiar legal-political cost: nobody knows what the concept actually means in practice. Although global discourses on policy and governance are typically infused with ambiguity, abstract concepts are of little use if they ignore the operational context that confronts the military in their application. This article places this intuitively appealing concept in context, and thus examines it in operational practice. Paying attention to this military practice is important as it demonstrates that meaningful human control is not the only, or the best, approach through which to characterize the human role and govern the challenges raised by autonomous weapons.},
	year = {2019}
}


